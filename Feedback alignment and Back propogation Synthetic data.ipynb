{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rng\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating the dataset \n",
    "inshape=30 #input size\n",
    "hiddenshape=20 # No of hidden units \n",
    "outshape=10 # Output size \n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "#Training parameters \n",
    "iterations=1000\n",
    "i=tf.Variable(0,name='loop_i')\n",
    "eta=tf.constant(0.0001,dtype=tf.float64)\n",
    "\n",
    "#Training data inputs\n",
    "x=tf.placeholder(tf.float64,[inshape,None], name = 'x')\n",
    "y=tf.placeholder(tf.float64,[outshape,None], name = 'y')\n",
    "\n",
    "#Linear network\n",
    "#B=tf.random_normal([hiddenshape,outshape],mean=0.0,stddev=1.0,dtype=tf.float64)\n",
    "#w_1=tf.random_normal([inshape,hiddenshape],mean=0.0,stddev=1.0,dtype=tf.float64)\n",
    "#w_2=tf.random_normal([hiddenshape,outshape],mean=0.0,stddev=1.0,dtype=tf.float64)\n",
    "\n",
    "#parameters for feedback alignment\n",
    "B = tf.Variable(rng.randn(hiddenshape, outshape))\n",
    "w_1 = tf.Variable(rng.randn(hiddenshape, inshape))\n",
    "w_2= tf.Variable(rng.randn(outshape, hiddenshape))\n",
    "\n",
    "hidout=tf.matmul(w_1,x)\n",
    "y_pred_fa=tf.matmul(w_2,hidout) \n",
    "\n",
    "e_fa=y_pred_fa-y\n",
    "cost_fa = tf.reduce_sum(tf.pow(e_fa, 2))/2\n",
    "Be=tf.matmul(B,e_fa)\n",
    "\n",
    "dLw1=tf.matmul(Be,tf.transpose(x))\n",
    "dLw2=tf.matmul(e_fa,tf.transpose(hidout))\n",
    "\n",
    "#Parameters for back propogation\n",
    "\n",
    "w1bp=tf.Variable(w_1.initialized_value())\n",
    "w2bp=tf.Variable(w_2.initialized_value())\n",
    "\n",
    "#1bp=w_1 \n",
    "#2bp=w_2\n",
    "\n",
    "hidout_bp=tf.matmul(w1bp,x)\n",
    "y_pred_bp=tf.matmul(w2bp,hidout_bp)\n",
    "\n",
    "e_bp=y_pred_bp-y\n",
    "cost_bp = tf.reduce_sum(tf.pow(e_bp, 2))/2\n",
    "We=tf.matmul(tf.transpose(w2bp),e_bp)\n",
    "\n",
    "dLw1bp=tf.matmul(We,tf.transpose(x))\n",
    "dLw2bp=tf.matmul(e_bp,tf.transpose(hidout_bp))\n",
    "\n",
    "#Training data generation\n",
    "#x=tf.random_normal([inshape,1],mean=0.0,stddev=1.0,dtype=tf.float64)\n",
    "#y=tf.matmul(T,x)\n",
    "#T=tf.random_normal([outshape,inshape],mean=0.0,stddev=1.0,dtype=tf.float64)\n",
    "T = rng.randn(outshape, inshape)\n",
    "def traindata(T, batch_size):\n",
    "    train_x = rng.randn(T.shape[1], batch_size)\n",
    "    train_y = np.dot(T, train_x)\n",
    "    return (train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 30)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updatefa(w_1,w_2):    \n",
    "    #gradients for a loss L \n",
    "    #Update\n",
    "    #w_1=tf.subtract(w_1,tf.multiply(eta,tf.transpose(dLw1)))\n",
    "    #w_2=tf.subtract(w_2,tf.multiply(eta,tf.transpose(dLw2))) \n",
    "    new_w1 = w_1.assign(w_1-tf.multiply(eta,dLw1))\n",
    "    new_w2 = w_2.assign(w_2-tf.multiply(eta,dLw2)) \n",
    "    return new_w1, new_w2 \n",
    "\n",
    "def updatebp(w1bp,w2bp):\n",
    "    new_w1bp=w1bp.assign(w1bp-tf.multiply(eta,dLw1bp))\n",
    "    new_w2bp=w2bp.assign(w2bp-tf.multiply(eta,dLw2bp))\n",
    "    return new_w1bp,new_w2bp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0 Feedbackalignment: 88230.362518\tBackprop:51074.654982\n",
      "iteration: 50 Feedbackalignment: 1668.047038\tBackprop:876.830632\n",
      "iteration: 100 Feedbackalignment: 646.484490\tBackprop:328.536859\n",
      "iteration: 150 Feedbackalignment: 263.779591\tBackprop:110.291161\n",
      "iteration: 200 Feedbackalignment: 113.483693\tBackprop:76.305585\n",
      "iteration: 250 Feedbackalignment: 86.808697\tBackprop:41.093944\n",
      "iteration: 300 Feedbackalignment: 53.994626\tBackprop:22.798180\n",
      "iteration: 350 Feedbackalignment: 46.572235\tBackprop:13.020742\n",
      "iteration: 400 Feedbackalignment: 25.121431\tBackprop:7.979320\n",
      "iteration: 450 Feedbackalignment: 21.362057\tBackprop:4.714387\n",
      "iteration: 500 Feedbackalignment: 16.430449\tBackprop:2.833432\n",
      "iteration: 550 Feedbackalignment: 8.992032\tBackprop:1.560447\n",
      "iteration: 600 Feedbackalignment: 4.500157\tBackprop:0.849602\n",
      "iteration: 650 Feedbackalignment: 4.103180\tBackprop:0.574027\n",
      "iteration: 700 Feedbackalignment: 2.966922\tBackprop:0.276222\n",
      "iteration: 750 Feedbackalignment: 1.922769\tBackprop:0.175140\n",
      "iteration: 800 Feedbackalignment: 0.838091\tBackprop:0.139077\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "costfa=[]\n",
    "costbp=[]\n",
    "storedot=[]\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for idx in range(iterations):\n",
    "        (train_x, train_y) = traindata(T, batch_size)\n",
    "        #sess.run(update(w_1, w_2), feed_dict = {x:train_x, y:train_y})\n",
    "        #for jj in range(batch_size):\n",
    "        sess.run(updatefa(w_1,w_2), feed_dict={x: train_x, y: train_y})\n",
    "        cfa = sess.run(cost_fa, feed_dict={x: train_x, y: train_y})\n",
    "        costfa.append(cfa)\n",
    "        sess.run(updatebp(w1bp,w2bp),feed_dict={x: train_x, y: train_y})\n",
    "        cbp = sess.run(cost_bp, feed_dict={x: train_x, y: train_y})\n",
    "        costbp.append(cbp)\n",
    "        if idx % 50==0:\n",
    "            print(\"iteration: %d Feedbackalignment: %f\\tBackprop:%f\"%(idx,cfa,cbp))\n",
    "            #delta_fa=sess.run(Be)\n",
    "            #delta_bp=sess.run(We)\n",
    "            #storedot.append(np.dot(delta_fa,delta_bp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.semilogy(costfa,'r',label=\"Random Feedback Alignment\")\n",
    "plt.semilogy(costbp,'b',labels=\"Back Propagation\")\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test[0].append(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
