{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "import numpy.random as rng\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import lfilter\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist=input_data.read_data_sets(\"MNIST_data/\",one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fo=open('/home/prashanth/cifar-10-batches-py/data_batch_1','rb')\n",
    "# dict=pickle.load(fo,encoding='bytes')\n",
    "# X=dict[b'data']\n",
    "# Y=dict[b'labels']\n",
    "# fo.close\n",
    "# X=X.reshape((len(X),3,32,32)).transpose(0,2,3,1).astype(\"uint8\")\n",
    "# Y=np.array(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_conv(height,width,inshape,outshape,input):\n",
    "    weights=tf.Variable(rng.randn(height,width,inshape,outshape), dtype = tf.float32,name='conv_weights') #constant\n",
    "    return(tf.nn.conv2d(input,weights,strides=[1,1,1,1],padding=\"SAME\"))\n",
    "\n",
    "def forward_pooling_layer(inp,window_size):\n",
    "    return(tf.nn.max_pool(value=inp,ksize=[1,window_size,window_size,1],strides=[1,1,1,1],padding=\"SAME\"))\n",
    "\n",
    "def flatten_forward(layer):\n",
    "    inp_list=layer.get_shape().as_list()\n",
    "    new_size = inp_list[-1] * inp_list[-2] * inp_list[-3]\n",
    "    return tf.reshape(layer,[-1,new_size]),new_size\n",
    "\n",
    "def fc_forward(layer,new_size,no_of_classes):\n",
    "    weights=tf.Variable(rng.randn(new_size,no_of_classes),dtype=tf.float32,name='fc_forward_weights') #constant\n",
    "    return tf.matmul(layer,weights)\n",
    "\n",
    "def fc_fc(rows,columns,layers):\n",
    "    weights=tf.Variable(rng.randn(rows,columns),dtype=tf.float32,name='fc_fc_weights')\n",
    "    return tf.matmul(layers,weights)\n",
    "\n",
    "def activation(layer):\n",
    "    return tf.nn.relu(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@tf.RegisterGradient(\"CustomConv\")\n",
    "def _conv2d(op,grad):\n",
    "    print(\"in override backprop\")\n",
    "    input = op.inputs[0]\n",
    "    filter = op.inputs[1]\n",
    "    in_shape = tf.shape(input)\n",
    "    f_shape = tf.shape(filter)\n",
    "    g_input = tf.nn.conv2d_backprop_input(input_sizes = in_shape, filter = filter, out_backprop = grad, strides = [1,1,1,1], padding = \"SAME\")\n",
    "    g_filter = tf.nn.conv2d_backprop_filter(input, filter_sizes = f_shape, out_backprop = grad, strides = [1,1,1,1], padding = \"SAME\")\n",
    "    return g_input, g_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#PARAMETERS\n",
    "num_epochs=1\n",
    "batch=100\n",
    "iterations=1000 #per epoch\n",
    "no_of_classes=10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random FeedBack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images_x=tf.placeholder(tf.float32,shape=(None,28*28),name='images')\n",
    "true_labels=tf.placeholder(tf.float32,shape=(None,10),name='true_labels')\n",
    "\n",
    "images=tf.reshape(images_x,[-1,28,28,1])\n",
    "#net_conv=forward_conv(filter_conv,images)\n",
    "\n",
    "#LAYER1\n",
    "filter_random1 = tf.Variable(rng.randn(5,5,1,20), dtype = tf.float32,name='random_filter1')\n",
    "\n",
    "g=tf.get_default_graph()\n",
    "with g.gradient_override_map({\"Conv2D\": \"CustomConv\"}):\n",
    "    net_conv=tf.nn.conv2d(images,filter_random1,strides=[1,1,1,1],padding=\"SAME\")\n",
    "\n",
    "net_pool= forward_pooling_layer(net_conv,2)\n",
    "net_act=activation(net_pool)\n",
    "\n",
    "#LAYER 2\n",
    "filter_random2 = tf.Variable(rng.randn(5,5,20,50), dtype = tf.float32,name='random_filter2')\n",
    "with g.gradient_override_map({\"Conv2D\":\"CustomConv\"}):\n",
    "    net_conv2=tf.nn.conv2d(net_act,filter_random2,strides=[1,1,1,1],padding=\"SAME\")\n",
    "net_pool2= forward_pooling_layer(net_conv2,2)\n",
    "net_act2=activation(net_pool2)\n",
    "    \n",
    "net_flatten,new_size=flatten_forward(net_act2)\n",
    "\n",
    "net_fc=fc_forward(net_flatten,new_size,500)\n",
    "net_act3=activation(net_fc)\n",
    "output=fc_fc(500,no_of_classes,net_act3)\n",
    "\n",
    "#compute loss\n",
    "\n",
    "cross_entropy=tf.nn.softmax_cross_entropy_with_logits(logits=\n",
    "                                  output,labels=true_labels)\n",
    "cost = tf.reduce_mean(cross_entropy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter_conv_bp=tf.Variable(rng.randn(5,5,3,16), dtype = tf.float32)\n",
    "\n",
    "#LAYER 1\n",
    "net_conv_bp= forward_conv(5,5,1,20,images) # height,width,inshape,outshape\n",
    "net_pool_bp = forward_pooling_layer(net_conv_bp,2) #output,windowsize\n",
    "net_act_bp=activation(net_pool_bp)\n",
    "\n",
    "#LAYER2\n",
    "net_conv_bp2 = forward_conv(5,5,20,50,net_act_bp) # height,width,inshape,outshape\n",
    "net_pool_bp2 = forward_pooling_layer(net_conv_bp2,2) #output,windowsize\n",
    "net_act_bp2=activation(net_pool_bp2)\n",
    "\n",
    "net_flatten_bp,new_size_bp=flatten_forward(net_act_bp2)\n",
    "net_fc_bp1=fc_forward(net_flatten_bp,new_size_bp,500)\n",
    "net_act_bp3=activation(net_fc_bp1)\n",
    "output_bp=fc_fc(500,no_of_classes,net_act_bp3)\n",
    "\n",
    "\n",
    "#cross_entropy_bp=tf.nn.softmax_cross_entropy_with_logits_v2(logits=\n",
    "#                                  output_bp,labels=true_labels)\n",
    "cross_entropy_bp=tf.nn.softmax_cross_entropy_with_logits(logits=\n",
    "                                  output_bp,labels=true_labels)\n",
    "cost_bp=tf.reduce_mean(cross_entropy_bp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy_fa=tf.reduce_mean(tf.cast(tf.equal(tf.argmax(output,1),tf.argmax(true_labels,1)),tf.float32))\n",
    "accuracy_bp=tf.reduce_mean(tf.cast(tf.equal(tf.argmax(output_bp,1),tf.argmax(true_labels,1)),tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in override backprop\n",
      "in override backprop\n"
     ]
    }
   ],
   "source": [
    "#BP gradients\n",
    "bp_grad = tf.gradients(cross_entropy_bp, images)\n",
    "\n",
    "override_grad = tf.gradients(cross_entropy, images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in override backprop\n",
      "in override backprop\n"
     ]
    }
   ],
   "source": [
    "train_op_bp = tf.train.MomentumOptimizer(learning_rate=0.00000001,momentum=0.9).minimize(cost_bp)\n",
    "train_op_fa=tf.train.MomentumOptimizer(learning_rate=0.00000001,momentum=0.9).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\t\tEPOCH NO: 1\n",
      "\n",
      "Iteration: 200\n",
      "BackPropagation: 92318.8 \tFeedback: 86760.76\n",
      "\n",
      "Iteration: 400\n",
      "BackPropagation: 67134.055 \tFeedback: 66135.88\n",
      "\n",
      "Iteration: 600\n",
      "BackPropagation: 50645.953 \tFeedback: 48436.29\n",
      "\n",
      "Iteration: 800\n",
      "BackPropagation: 46843.99 \tFeedback: 41623.39\n",
      "\n",
      "Iteration: 1000\n",
      "BackPropagation: 46419.94 \tFeedback: 35955.715\n",
      "\n",
      "At the end of EPOCH: 1\n",
      "Testing Accuracy:\n",
      "Back Propagation 0.2148 \tRandom Feedback: 0.2204\n"
     ]
    }
   ],
   "source": [
    "# Y_hot=np.eye(10)[Y]\n",
    "store_err_bp=[]\n",
    "store_err_fa=[]\n",
    "acc_fa=[]\n",
    "acc_bp=[]\n",
    "testing_fa=[]\n",
    "testing_bp=[]\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"\\n\\t\\t\\tEPOCH NO:\",epoch+1)\n",
    "        for count in range(iterations):\n",
    "            (inp_features,inp_labels)=mnist.train.next_batch(batch)\n",
    "            autobp_input=sess.run(bp_grad,feed_dict={images_x:inp_features,true_labels:inp_labels})\n",
    "            override_input=sess.run(override_grad,feed_dict={images_x:inp_features,true_labels:inp_labels})\n",
    "\n",
    "            sess.run(train_op_bp,feed_dict={images_x:inp_features,true_labels:inp_labels})\n",
    "            sess.run(train_op_fa,feed_dict={images_x:inp_features,true_labels:inp_labels})\n",
    "\n",
    "            entropy_bp=sess.run(cross_entropy_bp,feed_dict={images_x:inp_features,true_labels:inp_labels})\n",
    "            store_err_bp.append(np.mean(entropy_bp))\n",
    "            entropy_fa=sess.run(cross_entropy,feed_dict={images_x:inp_features,true_labels:inp_labels})\n",
    "            store_err_fa.append(np.mean(entropy_fa))\n",
    "\n",
    "            acc_fa.append(sess.run(accuracy_fa,feed_dict={images_x:inp_features,true_labels:inp_labels}))\n",
    "            acc_bp.append(sess.run(accuracy_bp,feed_dict={images_x:inp_features,true_labels:inp_labels}))\n",
    "\n",
    "\n",
    "            if (count+1)%200==0:\n",
    "                print(\"\\nIteration:\",count+1)\n",
    "#                 print(\"Cross Entropy loss\")\n",
    "                print(\"BackPropagation:\",sess.run(cost_bp,feed_dict={images_x:inp_features,true_labels:inp_labels}),\"\\tFeedback:\",sess.run(cost,feed_dict={images_x:inp_features,true_labels:inp_labels}))\n",
    "             \n",
    "        tst_fa=sess.run(accuracy_fa,feed_dict={images_x:mnist.validation.images,true_labels:mnist.validation.labels})\n",
    "        tst_bp=sess.run(accuracy_bp,feed_dict={images_x:mnist.validation.images,true_labels:mnist.validation.labels})\n",
    "\n",
    "        testing_fa.append(tst_fa)\n",
    "        testing_bp.append(tst_bp)\n",
    "        \n",
    "        print(\"\\nAt the end of EPOCH:\",epoch+1)\n",
    "        print(\"Testing Accuracy:\\nBack Propagation\",tst_bp,\"\\tRandom Feedback:\",tst_fa)    \n",
    "#         print(\"Training Accuracy:\\nBack Propagation\",train_bp\"\\tRandom Feedback:\",train_fa)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('Analysis.pkl','wb') as f:\n",
    "    pickle.dump([store_err_bp,store_err_fa,acc_fa,acc_bp,testing_fa,testing_bp],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
